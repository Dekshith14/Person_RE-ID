{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6bx1CttM0zc",
        "outputId": "a627d6de-ae57-4548-e1a4-63b94fc9b22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Ultralytics...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Install Ultralytics for YOLOv8\n",
        "print(\"Installing Ultralytics...\")\n",
        "!pip install ultralytics -q\n",
        "\n",
        "# 2. Install Torchreid\n",
        "print(\"Installing Torchreid...\")\n",
        "!pip install torchreid -q\n",
        "\n",
        "# 3. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import torchreid # For the appearance model\n",
        "import torch.nn.functional as F # For normalization\n",
        "print(\"All libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConv2d(nn.Module):\n",
        "    \"\"\"A simple 2D Convolutional block with BatchNorm and ReLU.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class GaitCNNLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN-LSTM architecture.\n",
        "    (This must be identical to your training notebook)\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=256, num_subjects=74, lstm_hidden_dim=512):\n",
        "        super(GaitCNNLSTM, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_subjects = num_subjects\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "\n",
        "        # --- CNN Backbone ---\n",
        "        self.conv1 = BasicConv2d(1, 32, 5, 1, 2)\n",
        "        self.conv2 = BasicConv2d(32, 32, 3, 1, 1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = BasicConv2d(32, 64, 3, 1, 1)\n",
        "        self.conv4 = BasicConv2d(64, 64, 3, 1, 1)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = BasicConv2d(64, 128, 3, 1, 1)\n",
        "        self.conv6 = BasicConv2d(128, 128, 3, 1, 1)\n",
        "        self.cnn_feature_size = 128 * 16 * 16\n",
        "\n",
        "        # --- LSTM Layer ---\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.cnn_feature_size,\n",
        "            hidden_size=self.lstm_hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # --- Head ---\n",
        "        self.fc1 = nn.Linear(self.lstm_hidden_dim, self.embedding_dim)\n",
        "        self.classifier = nn.Linear(self.embedding_dim, self.num_subjects)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _, _, _ = x.shape\n",
        "        x = x.view(batch_size * seq_len, 1, 64, 64)\n",
        "\n",
        "        x = self.conv1(x); x = self.conv2(x); x = self.maxpool1(x)\n",
        "        x = self.conv3(x); x = self.conv4(x); x = self.maxpool2(x)\n",
        "        x = self.conv5(x); x = self.conv6(x)\n",
        "\n",
        "        x = x.view(batch_size * seq_len, -1)\n",
        "\n",
        "        x = x.view(batch_size, seq_len, self.cnn_feature_size)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        x = h_n.squeeze(0)\n",
        "\n",
        "        embedding = self.fc1(x)\n",
        "        logits = self.classifier(embedding)\n",
        "\n",
        "        return logits, embedding\n",
        "\n",
        "print(\"Model class (GaitCNNLSTM) defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkTtX15SNTje",
        "outputId": "29b1b645-fbc8-4c45-a5ac-c44e5c622783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model class (GaitCNNLSTM) defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. SET UP DEVICE AND HYPERPARAMETERS ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Path to your new gallery file ---\n",
        "gallery_save_path = \"/content/drive/MyDrive/CAPSTONE/my_known_gallery.pth\"\n",
        "\n",
        "# --- Gait Model Hyperparameters ---\n",
        "GAIT_EMBEDDING_DIM = 256\n",
        "NUM_SUBJECTS = 74\n",
        "SEQ_LEN = 30\n",
        "\n",
        "# --- ⚠️ CRITICAL ⚠️ ---\n",
        "# You MUST tune this value manually.\n",
        "# Since we are using normalized embeddings, the distance is between 0 (identical) and 2 (opposite).\n",
        "# A good starting guess is 1.0.\n",
        "REID_THRESHOLD = 1.5 # <--- START WITH 1.0 AND TUNE\n",
        "# ----------------------\n",
        "\n",
        "# --- 2. Load GAIT Model (GaitCNNLSTM) ---\n",
        "print(\"Loading trained GaitCNNLSTM model...\")\n",
        "gait_model = GaitCNNLSTM(GAIT_EMBEDDING_DIM, NUM_SUBJECTS).to(device)\n",
        "gait_model_path = \"/content/drive/MyDrive/CAPSTONE/my_gait_cnnlstm.pth\"\n",
        "gait_model.load_state_dict(torch.load(gait_model_path))\n",
        "gait_model.eval()\n",
        "print(\"GaitCNNLSTM model loaded successfully.\")\n",
        "\n",
        "# --- 3. Load APPEARANCE Model (Torchreid OSNet) ---\n",
        "print(\"Loading pre-trained Appearance (OSNet) model from Torchreid...\")\n",
        "appearance_model = torchreid.models.build_model(\n",
        "    name='osnet_x1_0',\n",
        "    num_classes=751,\n",
        "    pretrained=True\n",
        ")\n",
        "appearance_model = appearance_model.to(device)\n",
        "appearance_model.eval()\n",
        "print(\"Appearance (OSNet) model loaded successfully. (Output dim: 512)\")\n",
        "\n",
        "# --- 4. Load TRACKING Model (YOLOv8-Seg) ---\n",
        "print(\"Loading YOLOv8-Seg model for tracking...\")\n",
        "yolo_seg_model = YOLO('yolov8n-seg.pt')\n",
        "yolo_seg_model.to(device)\n",
        "print(\"YOLOv8-Seg model loaded successfully.\")\n",
        "\n",
        "# --- 5. Load POSE Model (YOLOv8-Pose) ---\n",
        "print(\"Loading YOLOv8-Pose model...\")\n",
        "pose_model = YOLO('yolov8n-pose.pt')\n",
        "pose_model.to(device)\n",
        "print(\"YOLOv8-Pose model loaded successfully.\")\n",
        "\n",
        "# --- 6. Define ALL preprocessing transforms ---\n",
        "gait_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "appearance_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# --- 7. Load the Pre-Enrolled Gallery ---\n",
        "if os.path.exists(gallery_save_path):\n",
        "    gallery_of_known_people = torch.load(gallery_save_path)\n",
        "    print(f\"\\n✅ Successfully loaded {len(gallery_of_known_people)} known people from gallery.\")\n",
        "    print(f\"   Known IDs: {list(gallery_of_known_people.keys())}\")\n",
        "else:\n",
        "    gallery_of_known_people = {}\n",
        "    print(f\"\\n❌ WARNING: Gallery file not found at {gallery_save_path}. Will not be able to identify anyone.\")\n",
        "\n",
        "\n",
        "print(\"\\nAll models and transforms are ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FIyT_57NWGG",
        "outputId": "f53c135d-1ccd-4715-c00f-8a2873a7433b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading trained GaitCNNLSTM model...\n",
            "GaitCNNLSTM model loaded successfully.\n",
            "Loading pre-trained Appearance (OSNet) model from Torchreid...\n",
            "Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
            "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
            "Appearance (OSNet) model loaded successfully. (Output dim: 512)\n",
            "Loading YOLOv8-Seg model for tracking...\n",
            "YOLOv8-Seg model loaded successfully.\n",
            "Loading YOLOv8-Pose model...\n",
            "YOLOv8-Pose model loaded successfully.\n",
            "\n",
            "✅ Successfully loaded 3 known people from gallery.\n",
            "   Known IDs: ['Naman', 'Nishant', 'Aadishesh']\n",
            "\n",
            "All models and transforms are ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. ⚠️ UPDATE YOUR VIDEO PATHS ---\n",
        "video_path = '/content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/image.mp4' # <-- YOUR CROWDED VIDEO\n",
        "output_video_path = '/content/drive/MyDrive/CAPSTONE/mainfinal_finaltry.mp4'  # <-- OUTPUT\n",
        "# ------------------------------------\n",
        "\n",
        "# --- 2. Open input video and get properties ---\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video file {video_path}\")\n",
        "else:\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "    print(f\"Processing {total_frames} frames... Output will be saved to {output_video_path}\")\n",
        "\n",
        "# --- 3. Data structures for 3-WAY FUSED Re-ID ---\n",
        "tracked_gait_sequences = {}\n",
        "tracked_appearance_crops = {}\n",
        "tracked_body_ratios = {}\n",
        "# gallery_of_known_people is already loaded!\n",
        "track_id_to_person_id = {} # Format: {track_id: (person_id, match_distance)}\n",
        "\n",
        "# Helper function to get body ratios\n",
        "def get_body_ratios(kpts):\n",
        "    try:\n",
        "        nose_y = kpts[0, 1]\n",
        "        shoulder_y = (kpts[5, 1] + kpts[6, 1]) / 2.0\n",
        "        hip_y = (kpts[11, 1] + kpts[12, 1]) / 2.0\n",
        "        ankle_y = (kpts[15, 1] + kpts[16, 1]) / 2.0\n",
        "        total_height = ankle_y - nose_y\n",
        "        torso_height = hip_y - shoulder_y\n",
        "        leg_height = ankle_y - hip_y\n",
        "        if total_height > 1 and leg_height > 1:\n",
        "            ratio_1 = torso_height / total_height\n",
        "            ratio_2 = leg_height / total_height\n",
        "            return torch.tensor([ratio_1, ratio_2], dtype=torch.float32).to(device)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# --- 4. Start Video Loop ---\n",
        "try:\n",
        "    for _ in tqdm(range(total_frames), desc=\"Processing video\"):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # --- a. Run Tracker (YOLO-Seg) ---\n",
        "        results = yolo_seg_model.track(frame, persist=True, classes=0, verbose=False)\n",
        "\n",
        "        frame_draw_info = []\n",
        "\n",
        "        if results[0].masks is not None and results[0].boxes.id is not None:\n",
        "\n",
        "            masks = results[0].masks.data\n",
        "            boxes = results[0].boxes.data\n",
        "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "            for mask_tensor, box, track_id in zip(masks, boxes, track_ids):\n",
        "\n",
        "                x1, y1, x2, y2 = [int(i) for i in box[:4]]\n",
        "\n",
        "                # --- b. Process GAIT (Silhouette) ---\n",
        "                mask_np = mask_tensor.cpu().numpy() * 255\n",
        "                mask_pil = Image.fromarray(mask_np).convert('L')\n",
        "                silhouette_tensor = gait_transform(mask_pil).to(device)\n",
        "\n",
        "                # --- c. Process APPEARANCE (RGB Crop) ---\n",
        "                crop_img = frame[y1:y2, x1:x2]\n",
        "                crop_pil = Image.fromarray(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n",
        "                appearance_tensor = appearance_transform(crop_pil).to(device)\n",
        "\n",
        "                # --- d. Process POSE (Body Ratios) ---\n",
        "                pose_results = pose_model(crop_img, verbose=False)\n",
        "                body_ratio_tensor = None\n",
        "                if len(pose_results[0].keypoints.data) > 0:\n",
        "                    kpts = pose_results[0].keypoints.data[0].cpu().numpy()[:, :2]\n",
        "                    body_ratio_tensor = get_body_ratios(kpts)\n",
        "\n",
        "                # --- e. Store ALL THREE features ---\n",
        "                if track_id not in tracked_gait_sequences:\n",
        "                    tracked_gait_sequences[track_id] = []\n",
        "                    tracked_appearance_crops[track_id] = []\n",
        "                    tracked_body_ratios[track_id] = []\n",
        "\n",
        "                tracked_gait_sequences[track_id].append(silhouette_tensor)\n",
        "                tracked_appearance_crops[track_id].append(appearance_tensor)\n",
        "                if body_ratio_tensor is not None:\n",
        "                    tracked_body_ratios[track_id].append(body_ratio_tensor)\n",
        "\n",
        "                # --- f. Check for Full Sequence & Run FUSED Re-ID ---\n",
        "                if len(tracked_gait_sequences[track_id]) == SEQ_LEN:\n",
        "                    gait_sequence = torch.stack(tracked_gait_sequences[track_id], dim=0).unsqueeze(0)\n",
        "                    appearance_sequence = torch.stack(tracked_appearance_crops[track_id], dim=0)\n",
        "\n",
        "                    if len(tracked_body_ratios[track_id]) > 0:\n",
        "                        ratio_sequence = torch.stack(tracked_body_ratios[track_id], dim=0)\n",
        "                    else:\n",
        "                        ratio_sequence = torch.zeros((SEQ_LEN, 2)).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        _ , gait_embedding = gait_model(gait_sequence)\n",
        "                        appearance_embeddings = appearance_model(appearance_sequence)\n",
        "                        appearance_embedding = torch.mean(appearance_embeddings, dim=0, keepdim=True)\n",
        "                        body_ratio_embedding = torch.mean(ratio_sequence, dim=0, keepdim=True)\n",
        "\n",
        "                        # --- NORMALIZE (Must match enrollment) ---\n",
        "                        gait_embedding = F.normalize(gait_embedding, p=2, dim=1)\n",
        "                        appearance_embedding = F.normalize(appearance_embedding, p=2, dim=1)\n",
        "                        body_ratio_embedding = F.normalize(body_ratio_embedding, p=2, dim=1)\n",
        "\n",
        "                        final_embedding = torch.cat((gait_embedding, appearance_embedding, body_ratio_embedding), dim=1)\n",
        "                        # Shape is now [1, 256 + 512 + 2] = [1, 770]\n",
        "\n",
        "                    # --- MODIFIED Re-ID Logic ---\n",
        "                    is_new_person = True\n",
        "                    matched_id = None\n",
        "                    min_distance = float('inf')\n",
        "\n",
        "                    if len(gallery_of_known_people) > 0: # Check if gallery has people\n",
        "                        for person_name, known_embedding in gallery_of_known_people.items():\n",
        "                            distance = torch.cdist(final_embedding, known_embedding.to(device)).item()\n",
        "                            if distance < min_distance:\n",
        "                                min_distance = distance\n",
        "                                matched_id = person_name # e.g., \"Naman\"\n",
        "\n",
        "                    if min_distance < REID_THRESHOLD:\n",
        "                        is_new_person = False\n",
        "\n",
        "                    if is_new_person:\n",
        "                        # This is an unknown person. DO NOT add them to the gallery.\n",
        "                        track_id_to_person_id[track_id] = (f\"Track-{track_id}\", min_distance)\n",
        "                    else:\n",
        "                        # This is a known person.\n",
        "                        track_id_to_person_id[track_id] = (matched_id, min_distance)\n",
        "                    # --- END OF MODIFIED LOGIC ---\n",
        "\n",
        "                    tracked_gait_sequences[track_id] = []\n",
        "                    tracked_appearance_crops[track_id] = []\n",
        "                    tracked_body_ratios[track_id] = []\n",
        "\n",
        "                # --- g. Store Box Info for Drawing ---\n",
        "                person_id_mapping = track_id_to_person_id.get(track_id)\n",
        "                if person_id_mapping:\n",
        "                    display_id = person_id_mapping[0]\n",
        "                    match_distance = person_id_mapping[1]\n",
        "                else:\n",
        "                    display_id = f\"Track-{track_id}\"\n",
        "                    match_distance = float('inf')\n",
        "\n",
        "                frame_draw_info.append({\n",
        "                    'box': box,\n",
        "                    'display_id': display_id,\n",
        "                    'track_id': track_id,\n",
        "                    'match_distance': match_distance\n",
        "                })\n",
        "\n",
        "            # --- h. Conflict Resolution (Best-Match-Wins) ---\n",
        "            final_draw_list = []\n",
        "            person_id_assignments = {}\n",
        "\n",
        "            for info in frame_draw_info:\n",
        "                display_id = info['display_id']\n",
        "                match_distance = info['match_distance']\n",
        "                is_known = display_id in gallery_of_known_people # Check if it's \"Naman\" or \"Nishant\"\n",
        "\n",
        "                if is_known:\n",
        "                    if display_id not in person_id_assignments:\n",
        "                        person_id_assignments[display_id] = (match_distance, info)\n",
        "                    else:\n",
        "                        current_best_distance, _ = person_id_assignments[display_id]\n",
        "                        if match_distance < current_best_distance:\n",
        "                            _, old_info = person_id_assignments.pop(display_id)\n",
        "                            old_info['display_id'] = f\"Track-{old_info['track_id']}\" # Revert to track_id\n",
        "                            final_draw_list.append(old_info)\n",
        "                            person_id_assignments[display_id] = (match_distance, info)\n",
        "                        else:\n",
        "                            info['display_id'] = f\"Track-{info['track_id']}\" # Revert to track_id\n",
        "                            final_draw_list.append(info)\n",
        "                else:\n",
        "                    final_draw_list.append(info)\n",
        "\n",
        "            for best_distance, info in person_id_assignments.values():\n",
        "                final_draw_list.append(info)\n",
        "\n",
        "            # --- i. Draw all boxes at the end (using the final list) ---\n",
        "            for info in final_draw_list:\n",
        "                box = info['box']\n",
        "                display_id = info['display_id']\n",
        "                x1, y1, x2, y2 = [int(i) for i in box[:4]]\n",
        "\n",
        "                is_known = display_id in gallery_of_known_people\n",
        "                color = (0, 255, 0) if is_known else (0, 0, 255) # Green for known, Red for unknown\n",
        "                label = f\"{display_id}\" # Will show \"Naman\" or \"Track-123\"\n",
        "\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "                cv2.putText(frame, label, (x1, y1 - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        # --- j. Write the frame to the output video ---\n",
        "        writer.write(frame)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nProcessing interrupted by user.\")\n",
        "finally:\n",
        "    # --- 5. CRITICAL: Release all resources ---\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"\\nVideo processing finished. Output saved to: {output_video_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEfYEiOdNadu",
        "outputId": "1106f4de-fdbc-439b-e342-116994a9067d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1770 frames... Output will be saved to /content/drive/MyDrive/CAPSTONE/mainfinal_finaltry.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing video: 100%|██████████| 1770/1770 [02:11<00:00, 13.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Video processing finished. Output saved to: /content/drive/MyDrive/CAPSTONE/mainfinal_finaltry.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}