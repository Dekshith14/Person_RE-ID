{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xw_YIe_cYH",
        "outputId": "55abf163-775c-47bb-cf19-0ce4fa7d943f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing Ultralytics...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling Torchreid...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torchreid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. Install Libraries\n",
        "print(\"Installing Ultralytics...\")\n",
        "!pip install ultralytics -q\n",
        "print(\"Installing Torchreid...\")\n",
        "!pip install torchreid -q\n",
        "\n",
        "# 2. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import torchreid\n",
        "import torch.nn.functional as F\n",
        "print(\"All libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C30rG62y_17l",
        "outputId": "2212a900-a3f0-4ae8-c4f3-376b385dfa4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model class (GaitCNNLSTM) defined.\n"
          ]
        }
      ],
      "source": [
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x); x = self.bn(x); x = self.relu(x); return x\n",
        "\n",
        "class GaitCNNLSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim=256, num_subjects=74, lstm_hidden_dim=512):\n",
        "        super(GaitCNNLSTM, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_subjects = num_subjects\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.conv1 = BasicConv2d(1, 32, 5, 1, 2)\n",
        "        self.conv2 = BasicConv2d(32, 32, 3, 1, 1)\n",
        "        self.maxpool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = BasicConv2d(32, 64, 3, 1, 1)\n",
        "        self.conv4 = BasicConv2d(64, 64, 3, 1, 1)\n",
        "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = BasicConv2d(64, 128, 3, 1, 1)\n",
        "        self.conv6 = BasicConv2d(128, 128, 3, 1, 1)\n",
        "        self.cnn_feature_size = 128 * 16 * 16\n",
        "        self.lstm = nn.LSTM(self.cnn_feature_size, self.lstm_hidden_dim, 1, batch_first=True)\n",
        "        self.fc1 = nn.Linear(self.lstm_hidden_dim, self.embedding_dim)\n",
        "        self.classifier = nn.Linear(self.embedding_dim, self.num_subjects)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _, _, _ = x.shape\n",
        "        x = x.view(batch_size * seq_len, 1, 64, 64)\n",
        "        x = self.conv1(x); x = self.conv2(x); x = self.maxpool1(x)\n",
        "        x = self.conv3(x); x = self.conv4(x); x = self.maxpool2(x)\n",
        "        x = self.conv5(x); x = self.conv6(x)\n",
        "        x = x.view(batch_size * seq_len, -1)\n",
        "        x = x.view(batch_size, seq_len, self.cnn_feature_size)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "        x = h_n.squeeze(0)\n",
        "        embedding = self.fc1(x)\n",
        "        logits = self.classifier(embedding)\n",
        "        return logits, embedding\n",
        "\n",
        "print(\"Model class (GaitCNNLSTM) defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaqE6puIAGHL",
        "outputId": "71e9b94d-f507-4f09-a88f-4a9746283de4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading trained GaitCNNLSTM model...\n",
            "GaitCNNLSTM model loaded successfully.\n",
            "Loading pre-trained Appearance (OSNet) model from Torchreid...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LaG1EJpHrxdAxKnSCJ_i0u-nbxSAeiFY\n",
            "To: /root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\n",
            "100%|██████████| 10.9M/10.9M [00:00<00:00, 29.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded imagenet pretrained weights from \"/root/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
            "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
            "Appearance (OSNet) model loaded successfully. (Output dim: 512)\n",
            "Loading YOLOv8-Seg model for tracking...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt': 100% ━━━━━━━━━━━━ 6.7MB 303.2MB/s 0.0s\n",
            "YOLOv8-Seg model loaded successfully.\n",
            "Loading YOLOv8-Pose model...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt': 100% ━━━━━━━━━━━━ 6.5MB 266.5MB/s 0.0s\n",
            "YOLOv8-Pose model loaded successfully.\n",
            "\n",
            "All models and transforms are ready for enrollment.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. SET UP DEVICE AND HYPERPARAMETERS ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "GAIT_EMBEDDING_DIM = 256\n",
        "NUM_SUBJECTS = 74\n",
        "SEQ_LEN = 30\n",
        "\n",
        "# --- 2. Load GAIT Model (GaitCNNLSTM) ---\n",
        "print(\"Loading trained GaitCNNLSTM model...\")\n",
        "gait_model = GaitCNNLSTM(GAIT_EMBEDDING_DIM, NUM_SUBJECTS).to(device)\n",
        "gait_model_path = \"/content/drive/MyDrive/CAPSTONE/my_gait_cnnlstm.pth\"\n",
        "gait_model.load_state_dict(torch.load(gait_model_path))\n",
        "gait_model.eval()\n",
        "print(\"GaitCNNLSTM model loaded successfully.\")\n",
        "\n",
        "# --- 3. Load APPEARANCE Model (Torchreid OSNet) ---\n",
        "print(\"Loading pre-trained Appearance (OSNet) model from Torchreid...\")\n",
        "appearance_model = torchreid.models.build_model(name='osnet_x1_0', num_classes=751, pretrained=True)\n",
        "appearance_model = appearance_model.to(device)\n",
        "appearance_model.eval()\n",
        "print(\"Appearance (OSNet) model loaded successfully. (Output dim: 512)\")\n",
        "\n",
        "# --- 4. Load TRACKING Model (YOLOv8-Seg) ---\n",
        "print(\"Loading YOLOv8-Seg model for tracking...\")\n",
        "yolo_seg_model = YOLO('yolov8n-seg.pt')\n",
        "yolo_seg_model.to(device)\n",
        "print(\"YOLOv8-Seg model loaded successfully.\")\n",
        "\n",
        "# --- 5. Load POSE Model (YOLOv8-Pose) ---\n",
        "print(\"Loading YOLOv8-Pose model...\")\n",
        "pose_model = YOLO('yolov8n-pose.pt')\n",
        "pose_model.to(device)\n",
        "print(\"YOLOv8-Pose model loaded successfully.\")\n",
        "\n",
        "# --- 6. Define ALL preprocessing transforms ---\n",
        "gait_transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])\n",
        "appearance_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Helper function to get body ratios\n",
        "def get_body_ratios(kpts):\n",
        "    try:\n",
        "        nose_y = kpts[0, 1]; shoulder_y = (kpts[5, 1] + kpts[6, 1]) / 2.0\n",
        "        hip_y = (kpts[11, 1] + kpts[12, 1]) / 2.0; ankle_y = (kpts[15, 1] + kpts[16, 1]) / 2.0\n",
        "        total_height = ankle_y - nose_y; torso_height = hip_y - shoulder_y; leg_height = ankle_y - hip_y\n",
        "        if total_height > 1 and leg_height > 1:\n",
        "            return torch.tensor([torso_height / total_height, leg_height / total_height], dtype=torch.float32).to(device)\n",
        "    except Exception: pass\n",
        "    return None\n",
        "\n",
        "print(\"\\nAll models and transforms are ready for enrollment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sRkbaUSCJsP",
        "outputId": "32870450-bfdf-4da5-c5d8-ff606cd0fbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting enrollment for 3 people...\n",
            "\n",
            "--- Processing video for: Naman ---\n",
            "Video path: /content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/Naman.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Enrolling Naman: 100%|██████████| 1758/1758 [01:31<00:00, 19.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enrollment SUCCESS for Naman. Generated 48 embeddings and averaged them.\n",
            "   Final embedding shape: torch.Size([1, 770])\n",
            "\n",
            "--- Processing video for: Nishant ---\n",
            "Video path: /content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/Nishant.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Enrolling Nishant: 100%|██████████| 2288/2288 [01:51<00:00, 20.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enrollment SUCCESS for Nishant. Generated 58 embeddings and averaged them.\n",
            "   Final embedding shape: torch.Size([1, 770])\n",
            "\n",
            "--- Processing video for: Aadishesh ---\n",
            "Video path: /content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/Aadishesh.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Enrolling Aadishesh: 100%|██████████| 2030/2030 [01:37<00:00, 20.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enrollment SUCCESS for Aadishesh. Generated 46 embeddings and averaged them.\n",
            "   Final embedding shape: torch.Size([1, 770])\n",
            "\n",
            "--- ALL DONE ---\n",
            "Gallery with 3 people saved to: /content/drive/MyDrive/CAPSTONE/my_known_gallery.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# --- 1. ⚠️ DEFINE YOUR PEOPLE AND VIDEOS HERE ---\n",
        "# Format: (\"UNIQUE_PERSON_NAME\", \"/path/to/their/video.mp4\")\n",
        "people_to_enroll = [\n",
        "    (\"Naman\", \"/content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/Naman.mp4\"),\n",
        "    (\"Nishant\", \"/content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/Nishant.mp4\"),\n",
        "    (\"Aadishesh\", \"/content/drive/MyDrive/CAPSTONE/Custom_Dataset/house-1/Aadishesh.mp4\"),\n",
        "]\n",
        "# ------------------------------------------------\n",
        "\n",
        "# --- 2. DEFINE YOUR GALLERY SAVE PATH ---\n",
        "gallery_save_path = \"/content/drive/MyDrive/CAPSTONE/my_known_gallery.pth\"\n",
        "# ----------------------------------------\n",
        "\n",
        "gallery_of_known_people = {}\n",
        "print(f\"Starting enrollment for {len(people_to_enroll)} people...\")\n",
        "\n",
        "for person_name, video_path in people_to_enroll:\n",
        "    print(f\"\\n--- Processing video for: {person_name} ---\")\n",
        "    print(f\"Video path: {video_path}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Temporary storage for this person\n",
        "    tracked_gait_sequences = {}\n",
        "    tracked_appearance_crops = {}\n",
        "    tracked_body_ratios = {}\n",
        "\n",
        "    # Store all final embeddings for this person\n",
        "    all_final_embeddings = []\n",
        "\n",
        "    try:\n",
        "        for _ in tqdm(range(total_frames), desc=f\"Enrolling {person_name}\"):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # --- a. Run Tracker ---\n",
        "            # We assume only one person is in the enrollment video\n",
        "            results = yolo_seg_model.track(frame, persist=True, classes=0, verbose=False)\n",
        "\n",
        "            if results[0].masks is not None and results[0].boxes.id is not None:\n",
        "\n",
        "                # --- Find the main person (usually track_id 1, or the first one) ---\n",
        "                mask_tensor = results[0].masks.data[0]\n",
        "                box = results[0].boxes.data[0]\n",
        "                track_id = results[0].boxes.id.int().cpu().tolist()[0]\n",
        "\n",
        "                x1, y1, x2, y2 = [int(i) for i in box[:4]]\n",
        "\n",
        "                # --- b. Process GAIT ---\n",
        "                mask_np = mask_tensor.cpu().numpy() * 255\n",
        "                silhouette_tensor = gait_transform(Image.fromarray(mask_np).convert('L')).to(device)\n",
        "\n",
        "                # --- c. Process APPEARANCE ---\n",
        "                crop_img = frame[y1:y2, x1:x2]\n",
        "                appearance_tensor = appearance_transform(Image.fromarray(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))).to(device)\n",
        "\n",
        "                # --- d. Process POSE ---\n",
        "                pose_results = pose_model(crop_img, verbose=False)\n",
        "                body_ratio_tensor = None\n",
        "                if len(pose_results[0].keypoints.data) > 0:\n",
        "                    kpts = pose_results[0].keypoints.data[0].cpu().numpy()[:, :2]\n",
        "                    body_ratio_tensor = get_body_ratios(kpts)\n",
        "\n",
        "                # --- e. Store ALL features ---\n",
        "                if track_id not in tracked_gait_sequences:\n",
        "                    tracked_gait_sequences[track_id] = []\n",
        "                    tracked_appearance_crops[track_id] = []\n",
        "                    tracked_body_ratios[track_id] = []\n",
        "\n",
        "                tracked_gait_sequences[track_id].append(silhouette_tensor)\n",
        "                tracked_appearance_crops[track_id].append(appearance_tensor)\n",
        "                if body_ratio_tensor is not None:\n",
        "                    tracked_body_ratios[track_id].append(body_ratio_tensor)\n",
        "\n",
        "                # --- f. Check for Full Sequence & Create Embedding ---\n",
        "                if len(tracked_gait_sequences[track_id]) == SEQ_LEN:\n",
        "                    gait_sequence = torch.stack(tracked_gait_sequences[track_id], dim=0).unsqueeze(0)\n",
        "                    appearance_sequence = torch.stack(tracked_appearance_crops[track_id], dim=0)\n",
        "\n",
        "                    if len(tracked_body_ratios[track_id]) > 0:\n",
        "                        ratio_sequence = torch.stack(tracked_body_ratios[track_id], dim=0)\n",
        "                    else:\n",
        "                        ratio_sequence = torch.zeros((SEQ_LEN, 2)).to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        _ , gait_embedding = gait_model(gait_sequence)\n",
        "                        appearance_embedding = torch.mean(appearance_model(appearance_sequence), dim=0, keepdim=True)\n",
        "                        body_ratio_embedding = torch.mean(ratio_sequence, dim=0, keepdim=True)\n",
        "\n",
        "                        # --- NORMALIZE & FUSE ---\n",
        "                        gait_embedding = F.normalize(gait_embedding, p=2, dim=1)\n",
        "                        appearance_embedding = F.normalize(appearance_embedding, p=2, dim=1)\n",
        "                        body_ratio_embedding = F.normalize(body_ratio_embedding, p=2, dim=1)\n",
        "\n",
        "                        final_embedding = torch.cat((gait_embedding, appearance_embedding, body_ratio_embedding), dim=1)\n",
        "\n",
        "                        all_final_embeddings.append(final_embedding)\n",
        "                        \n",
        "\n",
        "                    # Clear sequences\n",
        "                    tracked_gait_sequences[track_id] = []\n",
        "                    tracked_appearance_crops[track_id] = []\n",
        "                    tracked_body_ratios[track_id] = []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "    finally:\n",
        "        cap.release()\n",
        "\n",
        "    # --- g. Create the \"Master\" Embedding ---\n",
        "    if len(all_final_embeddings) > 0:\n",
        "        master_embedding = torch.mean(torch.cat(all_final_embeddings, dim=0), dim=0, keepdim=True)\n",
        "        gallery_of_known_people[person_name] = master_embedding\n",
        "        print(f\"✅ Enrollment SUCCESS for {person_name}. Generated {len(all_final_embeddings)} embeddings and averaged them.\")\n",
        "        print(f\"   Final embedding shape: {master_embedding.shape}\")\n",
        "    else:\n",
        "        print(f\"❌ Enrollment FAILED for {person_name}. No full sequences were captured.\")\n",
        "\n",
        "# --- 3. Save the final gallery ---\n",
        "torch.save(gallery_of_known_people, gallery_save_path)\n",
        "print(f\"\\n--- ALL DONE ---\")\n",
        "print(f\"Gallery with {len(gallery_of_known_people)} people saved to: {gallery_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49RiUCEZmpxN",
        "outputId": "6fc5168f-d5f7-426d-f969-e886ca8eaccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Inspecting 3 Enrolled People ---\n",
            "\n",
            "Person ID: Naman\n",
            "  Embedding Shape:     torch.Size([1, 770])\n",
            "  Mean:      0.0227\n",
            "  Std Dev:   0.0492\n",
            "  Min:       -0.0922\n",
            "  Max:       0.8240\n",
            "  Slice (first 5 features): [   0.015047   -0.023165   -0.032798   -0.092175    0.023115]\n",
            "\n",
            "Person ID: Nishant\n",
            "  Embedding Shape:     torch.Size([1, 770])\n",
            "  Mean:      0.0233\n",
            "  Std Dev:   0.0511\n",
            "  Min:       -0.1315\n",
            "  Max:       0.7495\n",
            "  Slice (first 5 features): [   -0.03911   -0.045523    0.017388   -0.094984   -0.012199]\n",
            "\n",
            "Person ID: Aadishesh\n",
            "  Embedding Shape:     torch.Size([1, 770])\n",
            "  Mean:      0.0229\n",
            "  Std Dev:   0.0489\n",
            "  Min:       -0.0866\n",
            "  Max:       0.7856\n",
            "  Slice (first 5 features): [   0.022283    -0.03213   -0.050698   -0.083325    0.025126]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"--- Inspecting {len(gallery_of_known_people)} Enrolled People ---\")\n",
        "\n",
        "if not gallery_of_known_people:\n",
        "    print(\"Gallery is empty. No one is enrolled.\")\n",
        "else:\n",
        "    for person_name, embedding in gallery_of_known_people.items():\n",
        "        print(f\"\\nPerson ID: {person_name}\")\n",
        "\n",
        "        # Move to CPU for analysis, just in case it's on GPU\n",
        "        embedding_cpu = embedding.cpu()\n",
        "\n",
        "        print(f\"  Embedding Shape:     {embedding_cpu.shape}\")\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(f\"  Mean:      {embedding_cpu.mean().item():.4f}\")\n",
        "        print(f\"  Std Dev:   {embedding_cpu.std().item():.4f}\")\n",
        "        print(f\"  Min:       {embedding_cpu.min().item():.4f}\")\n",
        "        print(f\"  Max:       {embedding_cpu.max().item():.4f}\")\n",
        "\n",
        "        # Show a small slice to \"see\" the actual feature values\n",
        "        # We use .numpy() for a cleaner print\n",
        "        print(f\"  Slice (first 5 features): {embedding_cpu[0, :5].numpy()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
